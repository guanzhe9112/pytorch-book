{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADe7YDfyHs-h"
      },
      "source": [
        "## 3.2 autograd\n",
        "\n",
        "用Tensor训练网络很方便，但从上一小节最后的线性回归例子来看，反向传播过程需要手动实现。这对于像线性回归等较为简单的模型来说，还可以应付，但实际使用中经常出现非常复杂的网络结构，此时如果手动实现反向传播，不仅费时费力，而且容易出错，难以检查。torch.autograd就是为方便用户使用，而专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。\n",
        "\n",
        "计算图(Computation Graph)是现代深度学习框架如PyTorch和TensorFlow等的核心，其为高效自动求导算法——反向传播(Back Propogation)提供了理论支持，了解计算图在实际写程序过程中会有极大的帮助。本节将涉及一些基础的计算图知识，但并不要求读者事先对此有深入的了解。关于计算图的基础知识推荐阅读Christopher Olah的文章[^1]。\n",
        "\n",
        "[^1]: http://colah.github.io/posts/2015-08-Backprop/\n",
        "\n",
        "\n",
        "### 3.2.1 requires_grad\n",
        "PyTorch在autograd模块中实现了计算图的相关功能，autograd中的核心数据结构是Variable。从v0.4版本起，Variable和Tensor合并。我们可以认为需要求导(requires_grad)的tensor即Variable. autograd记录对tensor的操作记录用来构建计算图。\n",
        "\n",
        "Variable提供了大部分tensor支持的函数，但其不支持部分`inplace`函数，因这些函数会修改tensor自身，而在反向传播中，variable需要缓存原来的tensor来计算反向传播梯度。如果想要计算各个Variable的梯度，只需调用根节点variable的`backward`方法，autograd会自动沿着计算图反向传播，计算每一个叶子节点的梯度。\n",
        "\n",
        "`variable.backward(gradient=None, retain_graph=None, create_graph=None)`主要有如下参数：\n",
        "\n",
        "- grad_variables：形状与variable一致，对于`y.backward()`，grad_variables相当于链式法则${dz \\over dx}={dz \\over dy} \\times {dy \\over dx}$中的$\\textbf {dz} \\over \\textbf {dy}$。grad_variables也可以是tensor或序列。\n",
        "- retain_graph：反向传播需要缓存一些中间结果，反向传播之后，这些缓存就被清空，可通过指定这个参数不清空缓存，用来多次反向传播。\n",
        "- create_graph：对反向传播过程再次构建计算图，可通过`backward of backward`实现求高阶导数。\n",
        "\n",
        "上述描述可能比较抽象，如果没有看懂，不用着急，会在本节后半部分详细介绍，下面先看几个例子。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c-EoVV9YHs-q"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch as t\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Nd7FU9MPHs-r",
        "outputId": "4c772983-69a1-4403-d28e-f43de090820f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5343,  1.0587, -1.9582,  1.6431],\n",
              "        [-0.4034, -1.0234, -0.5388, -1.3284],\n",
              "        [ 0.2995,  0.2035,  1.0131, -0.8234]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "#在创建tensor的时候指定requires_grad\n",
        "a = t.randn(3,4, requires_grad=True)\n",
        "# 或者\n",
        "a = t.randn(3,4).requires_grad_()\n",
        "# 或者\n",
        "a = t.randn(3,4)\n",
        "a.requires_grad=True\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "AFYAsp1sHs-s",
        "outputId": "327a6801-5b28-4df5-96cc-52f647bed8be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "b = t.zeros(3,4).requires_grad_()\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "u3UK7z0WHs-t",
        "outputId": "121c1798-72aa-4dbc-a419-98ad5be97746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# 也可写成c = a + b\n",
        "c = a.add(b)\n",
        "c\n",
        "g = t.zeros_like(a)\n",
        "g[0][0] = 1\n",
        "c.backward(g)\n",
        "a.grad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "R8welpwCg1Lx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "s9bIis3THs-t"
      },
      "outputs": [],
      "source": [
        "d = c.sum()\n",
        "d.backward() # 反向传播"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6Hvcfw6BHs-u",
        "outputId": "de2b9fba-0be2-41e6-a948-df470dd9f341",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "d # d还是一个requires_grad=True的tensor,对它的操作需要慎重\n",
        "d.requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "Szgkp4o9Hs-u",
        "outputId": "9979c7c1-a424-43f5-fa73-d342e0b9a590"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "scrolled": false,
        "id": "_ebkDYU3Hs-u",
        "outputId": "f80cbc7c-4ae9-444e-c895-3831fb856a1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# 此处虽然没有指定c需要求导，但c依赖于a，而a需要求导，\n",
        "# 因此c的requires_grad属性会自动设为True\n",
        "a.requires_grad, b.requires_grad, c.requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qMkBMwLIHs-v",
        "outputId": "6d57d464-476f-43af-b32c-7ce63473358d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True, False)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# 由用户创建的variable属于叶子节点，对应的grad_fn是None\n",
        "a.is_leaf, b.is_leaf, c.is_leaf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eQOOZybKHs-v",
        "outputId": "187388f7-e62c-4ecf-d94e-ea274c51893e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:417.)\n",
            "  return self._grad\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# c.grad是None, 因c不是叶子节点，它的梯度是用来计算a的梯度\n",
        "# 所以虽然c.requires_grad = True,但其梯度计算完之后即被释放\n",
        "c.grad is None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVxzTAe9Hs-v"
      },
      "source": [
        "计算下面这个函数的导函数：\n",
        "$$\n",
        "y = x^2\\bullet e^x\n",
        "$$\n",
        "它的导函数是：\n",
        "$$\n",
        "{dy \\over dx} = 2x\\bullet e^x + x^2 \\bullet e^x\n",
        "$$\n",
        "来看看autograd的计算结果与手动求导计算结果的误差。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Z6HZm6NtHs-w"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    '''计算y'''\n",
        "    y = x**2 * t.exp(x)\n",
        "    return y\n",
        "\n",
        "def gradf(x):\n",
        "    '''手动求导函数'''\n",
        "    dx = 2*x*t.exp(x) + x**2*t.exp(x)\n",
        "    return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JdAwrtuYHs-w",
        "outputId": "182faffa-e400-4857-963e-4a582b74d3ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4.6747e-01, 3.8078e+00, 1.2324e-01, 1.0456e+01],\n",
              "        [1.1152e+01, 6.5800e-01, 2.5802e-01, 5.3651e-03],\n",
              "        [3.6443e+00, 4.0824e-01, 5.6150e+01, 4.4221e-02]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "x = t.randn(3,4, requires_grad = True)\n",
        "y = f(x)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jgGVvVQGHs-w",
        "outputId": "6682439d-cc8c-4f9c-ab23-bda7b5dda5e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  2.2460,  10.6285,  -0.4411,  24.2532],\n",
              "        [ 25.6031,   2.8487,  -0.4463,  -0.1357],\n",
              "        [ 10.2648,   2.0475, 104.2779,  -0.3294]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "y.backward(t.ones(y.size())) # gradient形状与y一致\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wGwsR6U7Hs-w",
        "outputId": "6d66acd2-cd97-4e26-ad8f-642e87c0d8ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  2.2460,  10.6285,  -0.4411,  24.2532],\n",
              "        [ 25.6031,   2.8487,  -0.4463,  -0.1357],\n",
              "        [ 10.2648,   2.0475, 104.2779,  -0.3294]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# autograd的计算结果与利用公式手动计算的结果一致\n",
        "gradf(x) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQyZwEsEHs-x"
      },
      "source": [
        "### 3.2.2 计算图\n",
        "\n",
        "PyTorch中`autograd`的底层采用了计算图，计算图是一种特殊的有向无环图（DAG），用于记录算子与变量之间的关系。一般用矩形表示算子，椭圆形表示变量。如表达式$ \\textbf {z = wx + b}$可分解为$\\textbf{y = wx}$和$\\textbf{z = y + b}$，其计算图如图3-3所示，图中`MUL`，`ADD`都是算子，$\\textbf{w}$，$\\textbf{x}$，$\\textbf{b}$即变量。\n",
        "\n",
        "![图3-3:computation graph](https://github.com/guanzhe9112/pytorch-book/blob/master/chapter03-tensor_and_autograd/imgs/com_graph.svg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSTvab7rHs-x"
      },
      "source": [
        "如上有向无环图中，$\\textbf{X}$和$\\textbf{b}$是叶子节点（leaf node），这些节点通常由用户自己创建，不依赖于其他变量。$\\textbf{z}$称为根节点，是计算图的最终目标。利用链式法则很容易求得各个叶子节点的梯度。\n",
        "$${\\partial z \\over \\partial b} = 1,\\space {\\partial z \\over \\partial y} = 1\\\\\n",
        "{\\partial y \\over \\partial w }= x,{\\partial y \\over \\partial x}= w\\\\\n",
        "{\\partial z \\over \\partial x}= {\\partial z \\over \\partial y} {\\partial y \\over \\partial x}=1 * w\\\\\n",
        "{\\partial z \\over \\partial w}= {\\partial z \\over \\partial y} {\\partial y \\over \\partial w}=1 * x\\\\\n",
        "$$\n",
        "而有了计算图，上述链式求导即可利用计算图的反向传播自动完成，其过程如图3-4所示。\n",
        "\n",
        "![图3-4：计算图的反向传播](https://github.com/guanzhe9112/pytorch-book/blob/master/chapter03-tensor_and_autograd/imgs/com_graph_backward.svg?raw=1)\n",
        "\n",
        "\n",
        "在PyTorch实现中，autograd会随着用户的操作，记录生成当前variable的所有操作，并由此建立一个有向无环图。用户每进行一个操作，相应的计算图就会发生改变。更底层的实现中，图中记录了操作`Function`，每一个变量在图中的位置可通过其`grad_fn`属性在图中的位置推测得到。在反向传播过程中，autograd沿着这个图从当前变量（根节点$\\textbf{z}$）溯源，可以利用链式求导法则计算所有叶子节点的梯度。每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个variable的梯度，这些函数的函数名通常以`Backward`结尾。下面结合代码学习autograd的实现细节。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "J_3aEEHCHs-x"
      },
      "outputs": [],
      "source": [
        "x = t.ones(1)\n",
        "b = t.rand(1, requires_grad = True)\n",
        "w = t.rand(1, requires_grad = True)\n",
        "y = w * x # 等价于y=w.mul(x)\n",
        "z = y + b # 等价于z=y.add(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "scrolled": true,
        "id": "_GLC1fpFHs-y",
        "outputId": "5d91b7b6-57ae-46d4-c7fe-862453d6b94a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False, True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "x.requires_grad, b.requires_grad, w.requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "l0mngL0KHs-y",
        "outputId": "881222ab-7329-4818-ca5f-57519dadfeb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# 虽然未指定y.requires_grad为True，但由于y依赖于需要求导的w\n",
        "# 故而y.requires_grad为True\n",
        "y.requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "uPApwCKDHs-y",
        "outputId": "ab0e7dd1-4e17-4d31-f6ec-0db221a5204f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "x.is_leaf, w.is_leaf, b.is_leaf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "UAeXSlekHs-z",
        "outputId": "84ddba95-4c80-4d38-d14c-65bd46bdbaf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False, False)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "y.is_leaf, z.is_leaf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "tTvRx2BrHs-z",
        "outputId": "b2d3ad33-fa75-4a92-87ad-4e5c242c7551",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AddBackward0 at 0x7f90ae5e9810>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# grad_fn可以查看这个variable的反向传播函数，\n",
        "# z是add函数的输出，所以它的反向传播函数是AddBackward\n",
        "z.grad_fn "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "scrolled": true,
        "id": "FCOvdYSqHs-z",
        "outputId": "bd7bbc3b-4270-4e1f-ccbd-89739fc40b27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((<MulBackward0 at 0x7f90ac891450>, 0),\n",
              " (<AccumulateGrad at 0x7f90ac891710>, 0))"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# next_functions保存grad_fn的输入，是一个tuple，tuple的元素也是Function\n",
        "# 第一个是y，它是乘法(mul)的输出，所以对应的反向传播函数y.grad_fn是MulBackward\n",
        "# 第二个是b，它是叶子节点，由用户创建，grad_fn为None，但是有\n",
        "print(b.grad_fn)\n",
        "z.grad_fn.next_functions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMBWiRlbHs-z",
        "outputId": "d9f2c273-e684-4386-8622-f492b42715fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# variable的grad_fn对应着和图中的function相对应\n",
        "z.grad_fn.next_functions[0][0] == y.grad_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "scrolled": true,
        "id": "wj3D7aLNHs-0",
        "outputId": "ca4b6454-87ca-47e1-fe41-e2450441966e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((<AccumulateGrad at 0x7f90ac8be610>, 0), (None, 0))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# 第一个是w，叶子节点，需要求导，梯度是累加的\n",
        "# 第二个是x，叶子节点，不需要求导，所以为None\n",
        "y.grad_fn.next_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "QIneO71yHs-0",
        "outputId": "4399a94f-6684-442a-dfc1-90f892b5f392",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# 叶子节点的grad_fn是None\n",
        "w.grad_fn,x.grad_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxpF6D8BHs-0"
      },
      "source": [
        "计算w的梯度的时候，需要用到x的数值(${\\partial y\\over \\partial w} = x $)，这些数值在前向过程中会保存成buffer，在计算完梯度之后会自动清空。为了能够多次反向传播需要指定`retain_graph`来保留这些buffer。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "scrolled": true,
        "id": "GplxkahgHs-0",
        "outputId": "fab7ccbf-9ad5-4324-bbd5-71d6cba4e31e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "# 使用retain_graph来保存buffer\n",
        "z.backward(retain_graph=True)\n",
        "w.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJv84oBmHs-0"
      },
      "outputs": [],
      "source": [
        "# 多次反向传播，梯度累加，这也就是w中AccumulateGrad标识的含义\n",
        "z.backward()\n",
        "w.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-LhfJ1NHs-1"
      },
      "source": [
        "PyTorch使用的是动态图，它的计算图在每次前向传播时都是从头开始构建，所以它能够使用Python控制语句（如for、if等）根据需求创建计算图。这点在自然语言处理领域中很有用，它意味着你不需要事先构建所有可能用到的图的路径，图在运行时才构建。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "LWM8UmLfHs-1",
        "outputId": "719b1b04-0b0e-491b-ebdd-38c7854ac31e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "def abs(x):\n",
        "    if x.data[0]>0: return x\n",
        "    else: return -x\n",
        "x = t.ones(1,requires_grad=True)\n",
        "y = abs(x)\n",
        "y.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "aLLM0upnHs-1",
        "outputId": "de64c2d0-1789-4035-966f-f0350fc97bf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.])\n"
          ]
        }
      ],
      "source": [
        "x = -1*t.ones(1)\n",
        "x = x.requires_grad_()\n",
        "y = abs(x)\n",
        "y.backward()\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "a1l9aWuzHs-1",
        "outputId": "8dba9264-443e-43ef-f664-307756ca97ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.], grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "wf9Dh8jpHs-1",
        "outputId": "9eba5e79-1371-41fe-eae9-c45274689cc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "PgABAWuEHs-2",
        "outputId": "bb7e5d6c-4257-4845-983a-3ee296b63c10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "x.requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "-M0e4CgXHs-2",
        "outputId": "b1ded0e8-8b66-4411-abdc-fbd69b89d89b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "x.requires_grad\n",
        "cc=x*3\n",
        "cc.requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "LvxLXOfTHs-2",
        "outputId": "2c24b39d-b678-4aa7-d3b3-3de49e296e1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 6., 3., 2.])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "def f(x):\n",
        "    result = 1\n",
        "    for ii in x:\n",
        "        if ii.item()>0: \n",
        "            result=ii*result\n",
        "    return result\n",
        "x = t.arange(-2,4,dtype=t.float32).requires_grad_()\n",
        "y = f(x) # y = x[3]*x[4]*x[5]\n",
        "y.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yDxeY-fHs-2"
      },
      "source": [
        "变量的`requires_grad`属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点`requires_grad`都是True。这其实很好理解，对于$ \\textbf{x}\\to \\textbf{y} \\to \\textbf{z}$，x.requires_grad = True，当需要计算$\\partial z \\over \\partial x$时，根据链式法则，$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$，自然也需要求$ \\frac{\\partial z}{\\partial y}$，所以y.requires_grad会被自动标为True. \n",
        "\n",
        "\n",
        "\n",
        "有些时候我们可能不希望autograd对tensor求导。认为求导需要缓存许多中间结构，增加额外的内存/显存开销，那么我们可以关闭自动求导。对于不需要反向传播的情景（如inference，即测试推理时），关闭自动求导可实现一定程度的速度提升，并节省约一半显存，因其不需要分配空间计算梯度。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "Eg5Eei8-Hs-2",
        "outputId": "a9bcaac5-6e38-4cdb-a94b-afd2f8cbf10d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "x = t.ones(1, requires_grad=True)\n",
        "w = t.rand(1, requires_grad=True)\n",
        "y = x * w\n",
        "# y依赖于w，而w.requires_grad = True\n",
        "x.requires_grad, w.requires_grad, y.requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "kvnTVGTXHs-3",
        "outputId": "23846a9a-7503-49a9-e82a-d534d65ad88d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False, True, False)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "with t.no_grad():\n",
        "    x = t.ones(1)\n",
        "    w = t.rand(1, requires_grad = True)\n",
        "    y = x * w\n",
        "# y依赖于w和x，虽然w.requires_grad = True，但是y的requires_grad依旧为False\n",
        "x.requires_grad, w.requires_grad, y.requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "HgYg-uOSHs-3"
      },
      "outputs": [],
      "source": [
        "t.no_grad??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "_M4U0q2WHs-3",
        "outputId": "c98daa55-1f8f-42e7-82b6-9258b5a8263b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(False, True, False)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "t.set_grad_enabled(False)\n",
        "x = t.ones(1)\n",
        "w = t.rand(1, requires_grad = True)\n",
        "y = x * w\n",
        "# y依赖于w和x，虽然w.requires_grad = True，但是y的requires_grad依旧为False\n",
        "x.requires_grad, w.requires_grad, y.requires_grad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "i9sDwY5QHs-3",
        "outputId": "1e8b1ba8-0d12-476a-daba-72c23ca2d357",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7f90acddb7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "# 恢复默认配置\n",
        "t.set_grad_enabled(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoRS1UhjHs-3"
      },
      "source": [
        "如果我们想要修改tensor的数值，但是又不希望被autograd记录，那么我么可以对tensor.data进行操作"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "G-LeHgAlHs-3",
        "outputId": "f2f956dd-68c9-4412-903b-3060a9e1c35c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ],
      "source": [
        "a = t.ones(3,4,requires_grad=True)\n",
        "b = t.ones(3,4,requires_grad=True)\n",
        "c = a * b\n",
        "\n",
        "a.data # 还是一个tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "5S3DxXh4Hs-4",
        "outputId": "7c6fe15e-7586-4cf0-f4bf-7e37fa551d51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "a.data.requires_grad # 但是已经是独立于计算图之外"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "n7KDi308Hs-4",
        "outputId": "324befd5-b619-47de-fc35-4d978781a020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False <built-in method type of Tensor object at 0x7f90ac80a170>\n"
          ]
        }
      ],
      "source": [
        "d = a.data.sigmoid_() # sigmoid_ 是个inplace操作，会修改a自身的值\n",
        "print(d.requires_grad, d.type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "jOcxnBf_Hs-4",
        "outputId": "2ab4a0f9-e609-4523-d952-0e278c3d3157",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n",
              "        [0.7311, 0.7311, 0.7311, 0.7311],\n",
              "        [0.7311, 0.7311, 0.7311, 0.7311]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "a "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIZMYK7_Hs-4"
      },
      "source": [
        "如果我们希望对tensor，但是又不希望被记录, 可以使用tensor.data 或者tensor.detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "1PHY2FmmHs-4",
        "outputId": "06711ede-f119-4c99-aa96-3c92e4332164",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "a.requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "QnX_k3LPHs-5",
        "outputId": "9e682cc8-f1b8-4111-8e3c-2c75318f22a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "# 近似于 tensor=a.data, 但是如果tensor被修改，backward可能会报错\n",
        "tensor = a.detach()\n",
        "tensor.requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "CLDY-jrmHs-5"
      },
      "outputs": [],
      "source": [
        "# 统计tensor的一些指标，不希望被记录\n",
        "mean = tensor.mean()\n",
        "std = tensor.std()\n",
        "maximum = tensor.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "M6tKzRpKHs-5"
      },
      "outputs": [],
      "source": [
        "tensor[0]=1\n",
        "# 下面会报错：　RuntimeError: one of the variables needed for gradient\n",
        "#             computation has been modified by an inplace operation\n",
        "#　因为 c=a*b, b的梯度取决于a，现在修改了tensor，其实也就是修改了a，梯度不再准确\n",
        "# c.sum().backward() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a73eLBJtHs-6"
      },
      "source": [
        "在反向传播过程中非叶子节点的导数计算完之后即被清空。若想查看这些变量的梯度，有两种方法：\n",
        "- 使用autograd.grad函数\n",
        "- 使用hook\n",
        "\n",
        "`autograd.grad`和`hook`方法都是很强大的工具，更详细的用法参考官方api文档，这里举例说明基础的使用。推荐使用`hook`方法，但是在实际使用中应尽量避免修改grad的值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "FdpXttSLHs-6",
        "outputId": "8aeb525f-3201-4661-d2a7-37c89caa62b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1.], requires_grad=True) tensor([0.3459, 0.0433, 0.7405], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ],
      "source": [
        "x = t.ones(3, requires_grad=True)\n",
        "w = t.rand(3, requires_grad=True)\n",
        "y = x * w\n",
        "# y依赖于w，而w.requires_grad = True\n",
        "z = y.sum()\n",
        "print(x, w)\n",
        "x.requires_grad, w.requires_grad, y.requires_grad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "IbSP__tdHs-7",
        "outputId": "c0ced585-f730-413b-eb5d-e622b681f780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:417.)\n",
            "  return self._grad\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.3459, 0.0433, 0.7405]), tensor([1., 1., 1.]), None)"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ],
      "source": [
        "# 非叶子节点grad计算完之后自动清空，y.grad是None\n",
        "z.backward()\n",
        "(x.grad, w.grad, y.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "_ARTg4BuHs-7",
        "outputId": "64a49f2d-a6f1-4077-f0da-626cea84cc6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1., 1., 1.]),)"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "# 第一种方法：使用grad获取中间变量的梯度\n",
        "x = t.ones(3, requires_grad=True)\n",
        "w = t.rand(3, requires_grad=True)\n",
        "y = x * w\n",
        "z = y.sum()\n",
        "# z对y的梯度，隐式调用backward()\n",
        "t.autograd.grad(z, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "shqppZhqHs-7",
        "outputId": "b4fd9ae3-d11b-49bc-cfb0-308ba527e58d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y的梯度： tensor([1., 1., 1.])\n"
          ]
        }
      ],
      "source": [
        "# 第二种方法：使用hook\n",
        "# hook是一个函数，输入是梯度，不应该有返回值\n",
        "def variable_hook(grad):\n",
        "    print('y的梯度：',grad)\n",
        "\n",
        "x = t.ones(3, requires_grad=True)\n",
        "w = t.rand(3, requires_grad=True)\n",
        "y = x * w\n",
        "# 注册hook\n",
        "hook_handle = y.register_hook(variable_hook)\n",
        "z = y.sum()\n",
        "z.backward()\n",
        "\n",
        "# 除非你每次都要用hook，否则用完之后记得移除hook\n",
        "hook_handle.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSZrs1ZmHs-7"
      },
      "source": [
        "最后再来看看variable中grad属性和backward函数`grad_variables`参数的含义，这里直接下结论：\n",
        "\n",
        "- variable $\\textbf{x}$的梯度是目标函数${f(x)} $对$\\textbf{x}$的梯度，$\\frac{df(x)}{dx} = (\\frac {df(x)}{dx_0},\\frac {df(x)}{dx_1},...,\\frac {df(x)}{dx_N})$，形状和$\\textbf{x}$一致。\n",
        "- 对于y.backward(grad_variables)中的grad_variables相当于链式求导法则中的$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$中的$\\frac{\\partial z}{\\partial y}$。z是目标函数，一般是一个标量，故而$\\frac{\\partial z}{\\partial y}$的形状与variable $\\textbf{y}$的形状一致。`z.backward()`在一定程度上等价于y.backward(grad_y)。`z.backward()`省略了grad_variables参数，是因为$z$是一个标量，而$\\frac{\\partial z}{\\partial z} = 1$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "scrolled": true,
        "id": "nh-Y2yyZHs-7",
        "outputId": "4b70a529-d7cc-4217-bbd3-70e43de06808",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ],
      "source": [
        "x = t.arange(0,3, requires_grad=True,dtype=t.float)\n",
        "y = x**2 + x*2\n",
        "z = y.sum()\n",
        "z.backward() # 从z开始反向传播\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "scrolled": true,
        "id": "OwfqSfdXHs-8",
        "outputId": "f7f7dd42-e161-46a5-9b43-e0697473a544",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "x = t.arange(0,3, requires_grad=True,dtype=t.float)\n",
        "y = x**2 + x*2\n",
        "z = y.sum()\n",
        "y_gradient = t.Tensor([1,1,1]) # dz/dy\n",
        "y.backward(y_gradient) #从y开始反向传播\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vIixtHWHs-8"
      },
      "source": [
        "另外值得注意的是，只有对variable的操作才能使用autograd，如果对variable的data直接进行操作，将无法使用反向传播。除了对参数初始化，一般我们不会修改variable.data的值。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDlmlX1OHs-8"
      },
      "source": [
        "在PyTorch中计算图的特点可总结如下：\n",
        "\n",
        "- autograd根据用户对variable的操作构建其计算图。对变量的操作抽象为`Function`。\n",
        "- 对于那些不是任何函数(Function)的输出，由用户创建的节点称为叶子节点，叶子节点的`grad_fn`为None。叶子节点中需要求导的variable，具有`AccumulateGrad`标识，因其梯度是累加的。\n",
        "- variable默认是不需要求导的，即`requires_grad`属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点`requires_grad`都为True。\n",
        "- variable的`volatile`属性默认为False，如果某一个variable的`volatile`属性被设为True，那么所有依赖它的节点`volatile`属性都为True。volatile属性为True的节点不会求导，volatile的优先级比`requires_grad`高。\n",
        "- 多次反向传播时，梯度是累加的。反向传播的中间缓存会被清空，为进行多次反向传播需指定`retain_graph`=True来保存这些缓存。\n",
        "- 非叶子节点的梯度计算完之后即被清空，可以使用`autograd.grad`或`hook`技术获取非叶子节点的值。\n",
        "- variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播\n",
        "- 反向传播函数`backward`的参数`grad_variables`可以看成链式求导的中间结果，如果是标量，可以省略，默认为1\n",
        "- PyTorch采用动态图设计，可以很方便地查看中间层的输出，动态的设计计算图结构。\n",
        "\n",
        "这些知识不懂大多数情况下也不会影响对pytorch的使用，但是掌握这些知识有助于更好的理解pytorch，并有效的避开很多陷阱"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z33tPfvHs-8"
      },
      "source": [
        "### 3.2.3 扩展autograd\n",
        "\n",
        "\n",
        "目前绝大多数函数都可以使用`autograd`实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办? 写一个`Function`，实现它的前向传播和反向传播代码，`Function`对应于计算图中的矩形， 它接收参数，计算并返回结果。下面给出一个例子。\n",
        "\n",
        "```python\n",
        "\n",
        "class Mul(Function):\n",
        "                                                            \n",
        "    @staticmethod\n",
        "    def forward(ctx, w, x, b, x_requires_grad = True):\n",
        "        ctx.x_requires_grad = x_requires_grad\n",
        "        ctx.save_for_backward(w,x)\n",
        "        output = w * x + b\n",
        "        return output\n",
        "        \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        w,x = ctx.saved_tensors\n",
        "        grad_w = grad_output * x\n",
        "        if ctx.x_requires_grad:\n",
        "            grad_x = grad_output * w\n",
        "        else:\n",
        "            grad_x = None\n",
        "        grad_b = grad_output * 1\n",
        "        return grad_w, grad_x, grad_b, None\n",
        "```\n",
        "\n",
        "分析如下：\n",
        "\n",
        "- 自定义的Function需要继承autograd.Function，没有构造函数`__init__`，forward和backward函数都是静态方法\n",
        "- backward函数的输出和forward函数的输入一一对应，backward函数的输入和forward函数的输出一一对应\n",
        "- backward函数的grad_output参数即t.autograd.backward中的`grad_variables`\n",
        "- 如果某一个输入不需要求导，直接返回None，如forward中的输入参数x_requires_grad显然无法对它求导，直接返回None即可\n",
        "- 反向传播可能需要利用前向传播的某些中间结果，需要进行保存，否则前向传播结束后这些对象即被释放\n",
        "\n",
        "Function的使用利用Function.apply(variable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "BipH-guTHs-8"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Function\n",
        "class MultiplyAdd(Function):\n",
        "                                                            \n",
        "    @staticmethod\n",
        "    def forward(ctx, w, x, b):                              \n",
        "        ctx.save_for_backward(w,x)\n",
        "        output = w * x + b\n",
        "        return output\n",
        "        \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):                         \n",
        "        w,x = ctx.saved_tensors\n",
        "        grad_w = grad_output * x\n",
        "        grad_x = grad_output * w\n",
        "        grad_b = grad_output * 1\n",
        "        return grad_w, grad_x, grad_b                       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "scrolled": true,
        "id": "j6KCgSfvHs-9",
        "outputId": "988b5feb-d469-4e53-df88-2cf9a52254cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1.]) tensor([0.6783, 0.3889, 0.0322], requires_grad=True) tensor([0.2125], requires_grad=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, tensor([1., 1., 1.]), tensor([3.]))"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ],
      "source": [
        "x = t.ones(3)\n",
        "w = t.rand(3, requires_grad = True)\n",
        "b = t.rand(1, requires_grad = True)\n",
        "# 开始前向传播\n",
        "z=MultiplyAdd.apply(w, x, b)\n",
        "# 开始反向传播\n",
        "z.backward(t.tensor([1,1,1]))\n",
        "\n",
        "print(x, w, b)\n",
        "# x不需要求导，中间过程还是会计算它的导数，但随后被清空\n",
        "x.grad, w.grad, b.grad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = t.ones(3)\n",
        "w = t.rand(3, 1, requires_grad = True)\n",
        "w * x\n",
        "print(w, x, w * x)"
      ],
      "metadata": {
        "id": "gECk4Yw5SJxf",
        "outputId": "d2dc34b2-a760-4bbf-a66e-5a1a87e4f679",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2516],\n",
            "        [0.9282],\n",
            "        [0.5508]], requires_grad=True) tensor([1., 1., 1.]) tensor([[0.2516, 0.2516, 0.2516],\n",
            "        [0.9282, 0.9282, 0.9282],\n",
            "        [0.5508, 0.5508, 0.5508]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "dhQ7HsmsHs-9",
        "outputId": "e32815a1-7205-44d7-a4f5-4166b5c70e8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1.]), tensor([0.3071], grad_fn=<MulBackward0>), tensor([1.]))"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ],
      "source": [
        "x = t.ones(1)\n",
        "w = t.rand(1, requires_grad = True)\n",
        "b = t.rand(1, requires_grad = True)\n",
        "#print('开始前向传播')\n",
        "z=MultiplyAdd.apply(w,x,b)\n",
        "#print('开始反向传播')\n",
        "\n",
        "# 调用MultiplyAdd.backward\n",
        "# 输出grad_w, grad_x, grad_b\n",
        "z.grad_fn.apply(t.ones(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j_6CFrRHs-9"
      },
      "source": [
        "之所以forward函数的输入是tensor，而backward函数的输入是variable，是为了实现高阶求导。backward函数的输入输出虽然是variable，但在实际使用时autograd.Function会将输入variable提取为tensor，并将计算结果的tensor封装成variable返回。在backward函数中，之所以也要对variable进行操作，是为了能够计算梯度的梯度（backward of backward）。下面举例说明，有关torch.autograd.grad的更详细使用请参照文档。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "anDQMmEIHs-9",
        "outputId": "33303f38-0cae-478f-f3c1-3c2e1b6ded17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([10.], grad_fn=<MulBackward0>),)"
            ]
          },
          "metadata": {},
          "execution_count": 165
        }
      ],
      "source": [
        "x = t.tensor([5], requires_grad=True,dtype=t.float)\n",
        "y = x ** 2\n",
        "grad_x = t.autograd.grad(y, x, create_graph=True)\n",
        "grad_x # dy/dx = 2 * x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "Y9qrdvEjHs-9",
        "outputId": "f9631128-6570-42b7-9bc0-2248fa2530aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([2.]),)"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ],
      "source": [
        "grad_grad_x = t.autograd.grad(grad_x[0],x)\n",
        "grad_grad_x # 二阶导数 d(2x)/dx = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTfnKUPWHs--"
      },
      "source": [
        "这种设计虽然能让`autograd`具有高阶求导功能，但其也限制了Tensor的使用，因autograd中反向传播的函数只能利用当前已经有的Variable操作。这个设计是在`0.2`版本新加入的，为了更好的灵活性，也为了兼容旧版本的代码，PyTorch还提供了另外一种扩展autograd的方法。PyTorch提供了一个装饰器`@once_differentiable`，能够在backward函数中自动将输入的variable提取成tensor，把计算结果的tensor自动封装成variable。有了这个特性我们就能够很方便的使用numpy/scipy中的函数，操作不再局限于variable所支持的操作。但是这种做法正如名字中所暗示的那样只能求导一次，它打断了反向传播图，不再支持高阶求导。\n",
        "\n",
        "\n",
        "上面所描述的都是新式Function，还有个legacy Function，可以带有`__init__`方法，`forward`和`backwad`函数也不需要声明为`@staticmethod`，但随着版本更迭，此类Function将越来越少遇到，在此不做更多介绍。\n",
        "\n",
        "此外在实现了自己的Function之后，还可以使用`gradcheck`函数来检测实现是否正确。`gradcheck`通过数值逼近来计算梯度，可能具有一定的误差，通过控制`eps`的大小可以控制容忍的误差。\n",
        "关于这部份的内容可以参考github上开发者们的讨论[^3]。\n",
        "\n",
        "[^3]: https://github.com/pytorch/pytorch/pull/1016"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY10JUY0Hs--"
      },
      "source": [
        "下面举例说明如何利用Function实现sigmoid Function。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "4s_JZO6cHs--"
      },
      "outputs": [],
      "source": [
        "class Sigmoid(Function):\n",
        "                                                             \n",
        "    @staticmethod\n",
        "    def forward(ctx, x): \n",
        "        output = 1 / (1 + t.exp(-x))\n",
        "        ctx.save_for_backward(output)\n",
        "        return output\n",
        "        \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output): \n",
        "        output,  = ctx.saved_tensors\n",
        "        grad_x = output * (1 - output) * grad_output\n",
        "        return grad_x                            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "bi5aUQJCHs--",
        "outputId": "7f70795b-df32-4d1c-fdc3-2039a567cedb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ],
      "source": [
        "# 采用数值逼近方式检验计算梯度的公式对不对\n",
        "test_input = t.randn(3,4, requires_grad=True).double()\n",
        "t.autograd.gradcheck(Sigmoid.apply, (test_input,), eps=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "TRP82_eXHs--",
        "outputId": "77f609d5-383a-497e-fbf0-86872fc292e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 loops, best of 5: 179 µs per loop\n",
            "100 loops, best of 5: 179 µs per loop\n",
            "100 loops, best of 5: 77.4 µs per loop\n"
          ]
        }
      ],
      "source": [
        "def f_sigmoid(x):\n",
        "    y = Sigmoid.apply(x)\n",
        "    y.backward(t.ones(x.size()))\n",
        "    \n",
        "def f_naive(x):\n",
        "    y =  1/(1 + t.exp(-x))\n",
        "    y.backward(t.ones(x.size()))\n",
        "    \n",
        "def f_th(x):\n",
        "    y = t.sigmoid(x)\n",
        "    y.backward(t.ones(x.size()))\n",
        "    \n",
        "x=t.randn(100, 100, requires_grad=True)\n",
        "%timeit -n 100 f_sigmoid(x)\n",
        "%timeit -n 100 f_naive(x)\n",
        "%timeit -n 100 f_th(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87JfCDGJHs-_"
      },
      "source": [
        "显然`f_sigmoid`要比单纯利用`autograd`加减和乘方操作实现的函数快不少，因为f_sigmoid的backward优化了反向传播的过程。另外可以看出系统实现的built-in接口(t.sigmoid)更快。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX5yX-WqHs-_"
      },
      "source": [
        "### 3.2.4 小试牛刀: 用Variable实现线性回归\n",
        "在上一节中讲解了利用tensor实现线性回归，在这一小节中，将讲解如何利用autograd/Variable实现线性回归，以此感受autograd的便捷之处。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OohQWg1hHs-_"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display \n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "MVf8MEnhHs-_"
      },
      "outputs": [],
      "source": [
        "# 设置随机数种子，为了在不同人电脑上运行时下面的输出一致\n",
        "t.manual_seed(1000) \n",
        "\n",
        "def get_fake_data(batch_size=8):\n",
        "    ''' 产生随机数据：y = x*2 + 3，加上了一些噪声'''\n",
        "    x = t.rand(batch_size, 1) * 5\n",
        "    y = x * 2 + 3 + t.randn(batch_size, 1)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "sf-V5u4ZHs-_",
        "outputId": "351fa3f7-9f1d-4162-af4b-0da65702d912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 1]) torch.Size([8, 1]) torch.Size([8])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f90ac199610>"
            ]
          },
          "metadata": {},
          "execution_count": 175
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# 来看看产生x-y分布是什么样的\n",
        "x, y = get_fake_data()\n",
        "plt.scatter(x.squeeze().numpy(), y.squeeze().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "scrolled": false,
        "id": "U6I06EU5Hs-_",
        "outputId": "9dccff84-7ebb-4fd9-9ad6-978e7af2b3a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.9473410844802856 3.040416955947876\n"
          ]
        }
      ],
      "source": [
        "# 随机初始化参数\n",
        "w = t.rand(1,1, requires_grad=True)\n",
        "b = t.zeros(1,1, requires_grad=True)\n",
        "losses = np.zeros(500)\n",
        "\n",
        "lr =0.005 # 学习率\n",
        "\n",
        "for ii in range(500):\n",
        "    x, y = get_fake_data(batch_size=32)\n",
        "    \n",
        "    # forward：计算loss\n",
        "    y_pred = x.mm(w) + b.expand_as(y)\n",
        "    loss = 0.5 * (y_pred - y) ** 2\n",
        "    loss = loss.sum()\n",
        "    losses[ii] = loss.item()\n",
        "    \n",
        "    # backward：手动计算梯度\n",
        "    loss.backward()\n",
        "    \n",
        "    # 更新参数\n",
        "    w.data.sub_(lr * w.grad.data)\n",
        "    b.data.sub_(lr * b.grad.data)\n",
        "    \n",
        "    # 梯度清零\n",
        "    w.grad.data.zero_()\n",
        "    b.grad.data.zero_()\n",
        "    \n",
        "    if ii%50 ==0:\n",
        "        # 画图\n",
        "        display.clear_output(wait=True)\n",
        "        x = t.arange(0, 6).view(-1, 1).float()\n",
        "        y = x.mm(w.data) + b.data.expand_as(x)\n",
        "        plt.plot(x.numpy(), y.numpy()) # predicted\n",
        "        \n",
        "        x2, y2 = get_fake_data(batch_size=20) \n",
        "        plt.scatter(x2.numpy(), y2.numpy()) # true data\n",
        "        \n",
        "        plt.xlim(0,5)\n",
        "        plt.ylim(0,13)   \n",
        "        plt.show()\n",
        "        plt.pause(0.5)\n",
        "        \n",
        "print(w.item(), b.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "H8JDeD1hHs_A",
        "outputId": "44add51f-56d7-4652-a793-4702ad331eb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5.0, 50.0)"
            ]
          },
          "metadata": {},
          "execution_count": 179
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(losses)\n",
        "plt.ylim(5,50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNnnM0TCHs_A"
      },
      "source": [
        "用autograd实现的线性回归最大的不同点就在于autograd不需要计算反向传播，可以自动计算微分。这点不单是在深度学习，在许多机器学习的问题中都很有用。另外需要注意的是在每次反向传播之前要记得先把梯度清零。\n",
        "\n",
        "本章主要介绍了PyTorch中两个基础底层的数据结构：Tensor和autograd中的Variable。Tensor是一个类似Numpy数组的高效多维数值运算数据结构，有着和Numpy相类似的接口，并提供简单易用的GPU加速。Variable是autograd封装了Tensor并提供自动求导技术的，具有和Tensor几乎一样的接口。`autograd`是PyTorch的自动微分引擎，采用动态计算图技术，能够快速高效的计算导数。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Torch optimizer linear regression\n"
      ],
      "metadata": {
        "id": "BN4fnEGCEwoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bBRQBBeqEv0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 设置随机数种子，为了在不同人电脑上运行时下面的输出一致\n",
        "t.manual_seed(1000) \n",
        "w_true = torch.Tensor([2,-2,2,1,-1]).reshape(-1, 1)\n",
        "def get_fake_data(batch_size=8):\n",
        "    ''' 产生随机数据：y = x*2 + 3，加上了一些噪声'''\n",
        "    x = t.rand(batch_size, 5) * 5\n",
        "    y = x.mm(w_true) + 3 #+ t.randn(batch_size, 1) \n",
        "    return x, y"
      ],
      "metadata": {
        "id": "Ff55vIdIYuCB"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = t.rand(5,1, requires_grad=True)\n",
        "b = t.zeros(1,1, requires_grad=True)\n",
        "losses = np.zeros(2000)\n",
        "\n",
        "lr = 0.0008 # 学习率\n",
        "\n",
        "for ii in range(2000):\n",
        "    x, y = get_fake_data(batch_size=64)\n",
        "    \n",
        "    # forward：计算loss\n",
        "    y_pred = x.mm(w) + b.expand_as(y)\n",
        "    loss = 0.5 * (y_pred - y) ** 2\n",
        "    loss = loss.sum()\n",
        "    if ii % 1000 == 0:\n",
        "      print(loss, w, b)\n",
        "    losses[ii] = loss.item()\n",
        "    \n",
        "    # backward：手动计算梯度\n",
        "    loss.backward()\n",
        "    # 更新参数\n",
        "    w.data.sub_(lr * w.grad.data)\n",
        "    b.data.sub_(lr * b.grad.data)\n",
        "    \n",
        "    # 梯度清零\n",
        "    w.grad.data.zero_()\n",
        "    b.grad.data.zero_()"
      ],
      "metadata": {
        "id": "xPyDKABEZFLj",
        "outputId": "510378bc-e635-41b2-d203-2b74a3971a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1358.7867, grad_fn=<SumBackward0>) tensor([[0.9572],\n",
            "        [0.9608],\n",
            "        [0.2808],\n",
            "        [0.2639],\n",
            "        [0.6401]], requires_grad=True) tensor([[0.]], requires_grad=True)\n",
            "tensor(0.0410, grad_fn=<SumBackward0>) tensor([[ 2.0092],\n",
            "        [-1.9909],\n",
            "        [ 2.0098],\n",
            "        [ 1.0099],\n",
            "        [-0.9904]], requires_grad=True) tensor([[2.8698]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w,b)"
      ],
      "metadata": {
        "id": "JOLmg_XCNW5z",
        "outputId": "a5670e66-8408-4662-aadc-3700a83b5877",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.0005],\n",
            "        [-1.9995],\n",
            "        [ 2.0004],\n",
            "        [ 1.0005],\n",
            "        [-0.9995]], requires_grad=True) tensor([[2.9943]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class linearRegression(torch.nn.Module):\n",
        "    def __init__(self, inputSize, outputSize):\n",
        "        super(linearRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "AhVtINbDPJcD"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputDim = 5       # takes variable 'x' \n",
        "outputDim = 1       # takes variable 'y'\n",
        "learningRate = 0.001\n",
        "epochs = 10000\n",
        "\n",
        "model = linearRegression(inputDim, outputDim)\n",
        "criterion = torch.nn.MSELoss() \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Converting inputs and labels to Variable\n",
        "    inputs, labels = get_fake_data(batch_size=64)\n",
        "\n",
        "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # get output from the model, given the inputs\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # get loss for the predicted output\n",
        "    loss = criterion(outputs, labels)\n",
        "    # get gradients w.r.t to parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    if epoch % 100 == 0:\n",
        "      print('epoch {}, loss {}'.format(epoch, loss.item()))"
      ],
      "metadata": {
        "id": "Y8fQAmxkdp9v",
        "outputId": "d7e85b29-13a0-43c3-ba48-0954a8a4053f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss 97.45208740234375\n",
            "epoch 100, loss 12.408723831176758\n",
            "epoch 200, loss 6.824019432067871\n",
            "epoch 300, loss 2.7260022163391113\n",
            "epoch 400, loss 1.7413157224655151\n",
            "epoch 500, loss 0.9508323669433594\n",
            "epoch 600, loss 0.6618268489837646\n",
            "epoch 700, loss 0.495278000831604\n",
            "epoch 800, loss 0.36196351051330566\n",
            "epoch 900, loss 0.4651297628879547\n",
            "epoch 1000, loss 0.3847980201244354\n",
            "epoch 1100, loss 0.39015597105026245\n",
            "epoch 1200, loss 0.45022597908973694\n",
            "epoch 1300, loss 0.35512715578079224\n",
            "epoch 1400, loss 0.39928698539733887\n",
            "epoch 1500, loss 0.37570250034332275\n",
            "epoch 1600, loss 0.38148316740989685\n",
            "epoch 1700, loss 0.36802181601524353\n",
            "epoch 1800, loss 0.4147531986236572\n",
            "epoch 1900, loss 0.34288302063941956\n",
            "epoch 2000, loss 0.3029418885707855\n",
            "epoch 2100, loss 0.23997437953948975\n",
            "epoch 2200, loss 0.2678609788417816\n",
            "epoch 2300, loss 0.28275036811828613\n",
            "epoch 2400, loss 0.34225699305534363\n",
            "epoch 2500, loss 0.2531697750091553\n",
            "epoch 2600, loss 0.23926451802253723\n",
            "epoch 2700, loss 0.26393118500709534\n",
            "epoch 2800, loss 0.20477968454360962\n",
            "epoch 2900, loss 0.24412226676940918\n",
            "epoch 3000, loss 0.17249692976474762\n",
            "epoch 3100, loss 0.2388542890548706\n",
            "epoch 3200, loss 0.2226344496011734\n",
            "epoch 3300, loss 0.277755469083786\n",
            "epoch 3400, loss 0.14330323040485382\n",
            "epoch 3500, loss 0.20917125046253204\n",
            "epoch 3600, loss 0.24616803228855133\n",
            "epoch 3700, loss 0.28381478786468506\n",
            "epoch 3800, loss 0.15231600403785706\n",
            "epoch 3900, loss 0.18035458028316498\n",
            "epoch 4000, loss 0.16159072518348694\n",
            "epoch 4100, loss 0.14796343445777893\n",
            "epoch 4200, loss 0.20518265664577484\n",
            "epoch 4300, loss 0.1809399425983429\n",
            "epoch 4400, loss 0.2043173909187317\n",
            "epoch 4500, loss 0.16354918479919434\n",
            "epoch 4600, loss 0.1882186084985733\n",
            "epoch 4700, loss 0.1427537202835083\n",
            "epoch 4800, loss 0.14992624521255493\n",
            "epoch 4900, loss 0.11821810156106949\n",
            "epoch 5000, loss 0.16129955649375916\n",
            "epoch 5100, loss 0.11828012019395828\n",
            "epoch 5200, loss 0.12022892385721207\n",
            "epoch 5300, loss 0.14294499158859253\n",
            "epoch 5400, loss 0.14087450504302979\n",
            "epoch 5500, loss 0.13593292236328125\n",
            "epoch 5600, loss 0.1399155557155609\n",
            "epoch 5700, loss 0.10613956302404404\n",
            "epoch 5800, loss 0.11274772882461548\n",
            "epoch 5900, loss 0.12651586532592773\n",
            "epoch 6000, loss 0.1280481070280075\n",
            "epoch 6100, loss 0.1090545803308487\n",
            "epoch 6200, loss 0.10505183041095734\n",
            "epoch 6300, loss 0.12005101889371872\n",
            "epoch 6400, loss 0.09263648837804794\n",
            "epoch 6500, loss 0.09529639780521393\n",
            "epoch 6600, loss 0.08136937767267227\n",
            "epoch 6700, loss 0.0985335260629654\n",
            "epoch 6800, loss 0.09360555559396744\n",
            "epoch 6900, loss 0.11883066594600677\n",
            "epoch 7000, loss 0.07912176847457886\n",
            "epoch 7100, loss 0.08996038883924484\n",
            "epoch 7200, loss 0.11981084942817688\n",
            "epoch 7300, loss 0.08898157626390457\n",
            "epoch 7400, loss 0.07759978622198105\n",
            "epoch 7500, loss 0.07460089772939682\n",
            "epoch 7600, loss 0.07088611274957657\n",
            "epoch 7700, loss 0.07058447599411011\n",
            "epoch 7800, loss 0.041855618357658386\n",
            "epoch 7900, loss 0.06228725612163544\n",
            "epoch 8000, loss 0.04959093779325485\n",
            "epoch 8100, loss 0.05689626932144165\n",
            "epoch 8200, loss 0.05050970986485481\n",
            "epoch 8300, loss 0.08879727125167847\n",
            "epoch 8400, loss 0.05639742314815521\n",
            "epoch 8500, loss 0.08729052543640137\n",
            "epoch 8600, loss 0.05225181579589844\n",
            "epoch 8700, loss 0.04832463711500168\n",
            "epoch 8800, loss 0.03695908188819885\n",
            "epoch 8900, loss 0.04853113740682602\n",
            "epoch 9000, loss 0.04175908491015434\n",
            "epoch 9100, loss 0.05503784120082855\n",
            "epoch 9200, loss 0.049438897520303726\n",
            "epoch 9300, loss 0.03706460818648338\n",
            "epoch 9400, loss 0.0470227375626564\n",
            "epoch 9500, loss 0.049386654049158096\n",
            "epoch 9600, loss 0.05582042783498764\n",
            "epoch 9700, loss 0.04041857272386551\n",
            "epoch 9800, loss 0.052455104887485504\n",
            "epoch 9900, loss 0.05048266425728798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)"
      ],
      "metadata": {
        "id": "wHYrTqVReKcU",
        "outputId": "061b6e11-d138-4b3d-a358-a4db89dea1d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear.weight tensor([[ 2.0633, -1.9385,  2.0623,  1.0622, -0.9379]])\n",
            "linear.bias tensor([2.1690])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "81Qb3NYVeaaz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "Autograd.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}